{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IEEE - Data Minification Private.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8EePbWTZhaQ"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBVBMBoNZoEo",
        "outputId": "ce9ae637-e0cb-43aa-c29e-272153a2bfab"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1x6xmQifUHQWZi7nDJs53do1G4q5aEnWI&export=download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x6xmQifUHQWZi7nDJs53do1G4q5aEnWI\n",
            "To: /content/ieee-fraud-detection.zip\n",
            "100% 124M/124M [00:01<00:00, 121MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35eNjFEZoOw",
        "outputId": "6a3b1816-f938-4f15-90ca-c63d74e41890"
      },
      "source": [
        "!unzip ieee-fraud-detection.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ieee-fraud-detection.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_identity.csv       \n",
            "  inflating: test_transaction.csv    \n",
            "  inflating: train_identity.csv      \n",
            "  inflating: train_transaction.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NaU2k-HZQmU"
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, warnings, datetime, math\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3nnsV3oZZCs"
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## -------------------\n",
        "## Memory Reducer\n",
        "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
        "# :verbose                                        # type: bool\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "## -------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoBjYzcUZfJ9"
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNOPtDF1Z8c8",
        "outputId": "3228a032-c9d3-4ea7-c54e-1e9fea97e950"
      },
      "source": [
        "########################### DATA LOAD\n",
        "#################################################################################\n",
        "print('Load Data')\n",
        "train_df = pd.read_csv('train_transaction.csv')\n",
        "test_df = pd.read_csv('test_transaction.csv')\n",
        "test_df['isFraud'] = 0\n",
        "\n",
        "train_identity = pd.read_csv('train_identity.csv')\n",
        "test_identity = pd.read_csv('test_identity.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfMJNbz2ujC8"
      },
      "source": [
        "\n",
        "\n",
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoALV882ezUH"
      },
      "source": [
        "import re\n",
        "names = test_identity.columns.tolist()\n",
        "for name in names:\n",
        "  if re.findall(r'id',name):\n",
        "    names[names.index(name)] = 'id_' + ' '.join(map(str, re.findall(r'\\d{2}',name)))\n",
        "#names[names.index(r' ')] = 'new_name'\n",
        "test_identity.columns = names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzvNiwtnaTpN",
        "outputId": "67374bca-86c8-4ebf-f67d-30ff6f156db1"
      },
      "source": [
        "########################### Base check\n",
        "#################################################################################\n",
        "\n",
        "for df in [train_df, test_df, train_identity, test_identity]:\n",
        "    original = df.copy()\n",
        "    df = reduce_mem_usage(df)\n",
        "\n",
        "    for col in list(df):\n",
        "        if df[col].dtype!='O':\n",
        "            if (df[col]-original[col]).sum()!=0:\n",
        "                df[col] = original[col]\n",
        "                print('Bad transformation', col)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
            "Bad transformation TransactionAmt\n",
            "Bad transformation dist1\n",
            "Bad transformation dist2\n",
            "Bad transformation C1\n",
            "Bad transformation C2\n",
            "Bad transformation C4\n",
            "Bad transformation C6\n",
            "Bad transformation C7\n",
            "Bad transformation C8\n",
            "Bad transformation C10\n",
            "Bad transformation C11\n",
            "Bad transformation C12\n",
            "Bad transformation C13\n",
            "Bad transformation D8\n",
            "Bad transformation D9\n",
            "Bad transformation V126\n",
            "Bad transformation V127\n",
            "Bad transformation V128\n",
            "Bad transformation V129\n",
            "Bad transformation V130\n",
            "Bad transformation V131\n",
            "Bad transformation V132\n",
            "Bad transformation V133\n",
            "Bad transformation V134\n",
            "Bad transformation V135\n",
            "Bad transformation V136\n",
            "Bad transformation V137\n",
            "Bad transformation V150\n",
            "Bad transformation V159\n",
            "Bad transformation V164\n",
            "Bad transformation V202\n",
            "Bad transformation V203\n",
            "Bad transformation V204\n",
            "Bad transformation V205\n",
            "Bad transformation V206\n",
            "Bad transformation V207\n",
            "Bad transformation V208\n",
            "Bad transformation V209\n",
            "Bad transformation V210\n",
            "Bad transformation V211\n",
            "Bad transformation V212\n",
            "Bad transformation V213\n",
            "Bad transformation V214\n",
            "Bad transformation V215\n",
            "Bad transformation V216\n",
            "Bad transformation V263\n",
            "Bad transformation V264\n",
            "Bad transformation V265\n",
            "Bad transformation V266\n",
            "Bad transformation V267\n",
            "Bad transformation V268\n",
            "Bad transformation V269\n",
            "Bad transformation V270\n",
            "Bad transformation V271\n",
            "Bad transformation V272\n",
            "Bad transformation V273\n",
            "Bad transformation V274\n",
            "Bad transformation V275\n",
            "Bad transformation V276\n",
            "Bad transformation V277\n",
            "Bad transformation V278\n",
            "Bad transformation V306\n",
            "Bad transformation V307\n",
            "Bad transformation V308\n",
            "Bad transformation V309\n",
            "Bad transformation V310\n",
            "Bad transformation V311\n",
            "Bad transformation V312\n",
            "Bad transformation V313\n",
            "Bad transformation V314\n",
            "Bad transformation V315\n",
            "Bad transformation V316\n",
            "Bad transformation V317\n",
            "Bad transformation V318\n",
            "Bad transformation V319\n",
            "Bad transformation V320\n",
            "Bad transformation V321\n",
            "Bad transformation V332\n",
            "Bad transformation V334\n",
            "Bad transformation V335\n",
            "Bad transformation V336\n",
            "Mem. usage decreased to 473.07 Mb (68.9% reduction)\n",
            "Bad transformation TransactionAmt\n",
            "Bad transformation dist1\n",
            "Bad transformation dist2\n",
            "Bad transformation C1\n",
            "Bad transformation C2\n",
            "Bad transformation C11\n",
            "Bad transformation C12\n",
            "Bad transformation D8\n",
            "Bad transformation D9\n",
            "Bad transformation V126\n",
            "Bad transformation V127\n",
            "Bad transformation V128\n",
            "Bad transformation V129\n",
            "Bad transformation V130\n",
            "Bad transformation V131\n",
            "Bad transformation V132\n",
            "Bad transformation V133\n",
            "Bad transformation V134\n",
            "Bad transformation V135\n",
            "Bad transformation V136\n",
            "Bad transformation V137\n",
            "Bad transformation V150\n",
            "Bad transformation V160\n",
            "Bad transformation V161\n",
            "Bad transformation V162\n",
            "Bad transformation V163\n",
            "Bad transformation V166\n",
            "Bad transformation V202\n",
            "Bad transformation V203\n",
            "Bad transformation V204\n",
            "Bad transformation V205\n",
            "Bad transformation V206\n",
            "Bad transformation V207\n",
            "Bad transformation V208\n",
            "Bad transformation V209\n",
            "Bad transformation V210\n",
            "Bad transformation V211\n",
            "Bad transformation V212\n",
            "Bad transformation V213\n",
            "Bad transformation V214\n",
            "Bad transformation V215\n",
            "Bad transformation V216\n",
            "Bad transformation V263\n",
            "Bad transformation V264\n",
            "Bad transformation V265\n",
            "Bad transformation V266\n",
            "Bad transformation V267\n",
            "Bad transformation V268\n",
            "Bad transformation V269\n",
            "Bad transformation V270\n",
            "Bad transformation V271\n",
            "Bad transformation V272\n",
            "Bad transformation V273\n",
            "Bad transformation V274\n",
            "Bad transformation V275\n",
            "Bad transformation V276\n",
            "Bad transformation V277\n",
            "Bad transformation V278\n",
            "Bad transformation V306\n",
            "Bad transformation V307\n",
            "Bad transformation V308\n",
            "Bad transformation V309\n",
            "Bad transformation V310\n",
            "Bad transformation V311\n",
            "Bad transformation V312\n",
            "Bad transformation V313\n",
            "Bad transformation V314\n",
            "Bad transformation V315\n",
            "Bad transformation V316\n",
            "Bad transformation V317\n",
            "Bad transformation V318\n",
            "Bad transformation V319\n",
            "Bad transformation V320\n",
            "Bad transformation V321\n",
            "Bad transformation V331\n",
            "Bad transformation V332\n",
            "Bad transformation V333\n",
            "Bad transformation V334\n",
            "Bad transformation V335\n",
            "Bad transformation V336\n",
            "Bad transformation V337\n",
            "Bad transformation V338\n",
            "Bad transformation V339\n",
            "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n",
            "Bad transformation id_11\n",
            "Mem. usage decreased to 25.44 Mb (42.7% reduction)\n",
            "Bad transformation id_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE3xeWZKaqZt"
      },
      "source": [
        "########################### TransactionDT\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
        "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
        "\n",
        "for df in [train_df, test_df]:\n",
        "    \n",
        "    # Temporary variables for aggregation\n",
        "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
        "    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
        "    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
        "    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
        "    \n",
        "    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
        "    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n",
        "    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
        "    df['DT_week_month'] = (df['DT'].dt.day)/7\n",
        "    df['DT_week_month'] = df['DT_week_month'].apply(lambda x: math.ceil(x))\n",
        "\n",
        "    # Possible solo feature\n",
        "    df['is_december'] = df['DT'].dt.month\n",
        "    df['is_december'] = (df['is_december']==12).astype(np.int8)\n",
        "\n",
        "    # Holidays\n",
        "    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
        "\n",
        "# Total transactions per timeblock\n",
        "for col in ['DT_M','DT_W','DT_D']:\n",
        "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "    fq_encode = temp_df[col].value_counts().to_dict()\n",
        "            \n",
        "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
        "    test_df[col+'_total']  = test_df[col].map(fq_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHy37dCWaq9b",
        "outputId": "1d5c5b28-7248-4339-8013-d2174065ca9c"
      },
      "source": [
        "########################### card4, card6, ProductCD\n",
        "#################################################################################\n",
        "# Converting Strings to ints(or floats if nan in column) using frequency encoding\n",
        "# We will be able to use these columns as category or as numerical feature\n",
        "\n",
        "for col in ['card4', 'card6', 'ProductCD']:\n",
        "    print('Encoding', col)\n",
        "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
        "    train_df[col] = train_df[col].map(col_encoded)\n",
        "    test_df[col]  = test_df[col].map(col_encoded)\n",
        "    print(col_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding card4\n",
            "{'visa': 719649, 'mastercard': 347386, 'american express': 16009, 'discover': 9524}\n",
            "Encoding card6\n",
            "{'debit': 824959, 'credit': 267648, 'debit or credit': 30, 'charge card': 16}\n",
            "Encoding ProductCD\n",
            "{'W': 800657, 'C': 137785, 'R': 73346, 'H': 62397, 'S': 23046}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piQ2z7pDatpG",
        "outputId": "95663b9a-0a09-4f74-ed33-99f273c9e638"
      },
      "source": [
        "########################### M columns\n",
        "#################################################################################\n",
        "# Converting Strings to ints(or floats if nan in column)\n",
        "\n",
        "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
        "    train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
        "    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
        "\n",
        "for col in ['M4']:\n",
        "    print('Encoding', col)\n",
        "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
        "    train_df[col] = train_df[col].map(col_encoded)\n",
        "    test_df[col]  = test_df[col].map(col_encoded)\n",
        "    print(col_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding M4\n",
            "{'M0': 357789, 'M2': 122947, 'M1': 97306}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxJ2hvx_esk7",
        "outputId": "b5cdedf1-b2c0-4620-ba3b-54fa9ac6f2a5"
      },
      "source": [
        "train_identity['id_34']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         match_status:2\n",
              "1         match_status:1\n",
              "2                    NaN\n",
              "3                    NaN\n",
              "4         match_status:2\n",
              "               ...      \n",
              "144228               NaN\n",
              "144229    match_status:2\n",
              "144230               NaN\n",
              "144231    match_status:2\n",
              "144232               NaN\n",
              "Name: id_34, Length: 144233, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAkgNHZtav4I"
      },
      "source": [
        "########################### Identity columns\n",
        "#################################################################################\n",
        "\n",
        "def minify_identity_df(df):\n",
        "\n",
        "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
        "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
        "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
        "\n",
        "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
        "\n",
        "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
        "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
        "\n",
        "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
        "\n",
        "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
        "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
        "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
        "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
        "\n",
        "    df['id_34'] = df['id_34'].fillna(':0')\n",
        "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
        "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
        "    \n",
        "    df['id_33'] = df['id_33'].fillna('0x0')\n",
        "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
        "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
        "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
        "\n",
        "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
        "    return df\n",
        "\n",
        "train_identity = minify_identity_df(train_identity)\n",
        "test_identity = minify_identity_df(test_identity)\n",
        "\n",
        "for col in ['id_33']:\n",
        "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
        "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(train_identity[col])+list(test_identity[col]))\n",
        "    train_identity[col] = le.transform(train_identity[col])\n",
        "    test_identity[col]  = le.transform(test_identity[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3deeYGnSazEw"
      },
      "source": [
        "########################### Deltas\n",
        "\n",
        "for df in [train_df, test_df]:\n",
        "    for col in ['D'+str(i) for i in range(1,16) if i!=9]: \n",
        "        new_col = 'uid_td_'+str(col)\n",
        "        df[new_col] = df[col].fillna(0).astype(int)\n",
        "        df[new_col] = df[new_col].apply(lambda x: pd.Timedelta(x, unit='D'))\n",
        "        df[new_col] = (df['DT'] - df[new_col]).dt.date\n",
        "        df[new_col] = df[new_col].astype(str)\n",
        "        df[new_col] = np.where(df[col].isna(), np.nan, df[new_col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICC8TW79a1gH",
        "outputId": "a9467bf5-2436-4a58-fed8-08743383e90d"
      },
      "source": [
        "########################### Final check\n",
        "#################################################################################\n",
        "\n",
        "for df in [train_df, test_df, train_identity, test_identity]:\n",
        "    original = df.copy()\n",
        "    df = reduce_mem_usage(df)\n",
        "\n",
        "    for col in list(df):\n",
        "        if df[col].dtype!='O':\n",
        "            if (df[col]-original[col]).sum()!=0:\n",
        "                df[col] = original[col]\n",
        "                print('Bad transformation', col)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 585.15 Mb (32.4% reduction)\n",
            "Bad transformation TransactionAmt\n",
            "Bad transformation dist1\n",
            "Bad transformation dist2\n",
            "Bad transformation C1\n",
            "Bad transformation C2\n",
            "Bad transformation C4\n",
            "Bad transformation C6\n",
            "Bad transformation C7\n",
            "Bad transformation C8\n",
            "Bad transformation C10\n",
            "Bad transformation C11\n",
            "Bad transformation C12\n",
            "Bad transformation C13\n",
            "Bad transformation D8\n",
            "Bad transformation D9\n",
            "Bad transformation V126\n",
            "Bad transformation V127\n",
            "Bad transformation V128\n",
            "Bad transformation V129\n",
            "Bad transformation V130\n",
            "Bad transformation V131\n",
            "Bad transformation V132\n",
            "Bad transformation V133\n",
            "Bad transformation V134\n",
            "Bad transformation V135\n",
            "Bad transformation V136\n",
            "Bad transformation V137\n",
            "Bad transformation V150\n",
            "Bad transformation V159\n",
            "Bad transformation V164\n",
            "Bad transformation V202\n",
            "Bad transformation V203\n",
            "Bad transformation V204\n",
            "Bad transformation V205\n",
            "Bad transformation V206\n",
            "Bad transformation V207\n",
            "Bad transformation V208\n",
            "Bad transformation V209\n",
            "Bad transformation V210\n",
            "Bad transformation V211\n",
            "Bad transformation V212\n",
            "Bad transformation V213\n",
            "Bad transformation V214\n",
            "Bad transformation V215\n",
            "Bad transformation V216\n",
            "Bad transformation V263\n",
            "Bad transformation V264\n",
            "Bad transformation V265\n",
            "Bad transformation V266\n",
            "Bad transformation V267\n",
            "Bad transformation V268\n",
            "Bad transformation V269\n",
            "Bad transformation V270\n",
            "Bad transformation V271\n",
            "Bad transformation V272\n",
            "Bad transformation V273\n",
            "Bad transformation V274\n",
            "Bad transformation V275\n",
            "Bad transformation V276\n",
            "Bad transformation V277\n",
            "Bad transformation V278\n",
            "Bad transformation V306\n",
            "Bad transformation V307\n",
            "Bad transformation V308\n",
            "Bad transformation V309\n",
            "Bad transformation V310\n",
            "Bad transformation V311\n",
            "Bad transformation V312\n",
            "Bad transformation V313\n",
            "Bad transformation V314\n",
            "Bad transformation V315\n",
            "Bad transformation V316\n",
            "Bad transformation V317\n",
            "Bad transformation V318\n",
            "Bad transformation V319\n",
            "Bad transformation V320\n",
            "Bad transformation V321\n",
            "Bad transformation V332\n",
            "Bad transformation V334\n",
            "Bad transformation V335\n",
            "Bad transformation V336\n",
            "Bad transformation DT\n",
            "Mem. usage decreased to 509.80 Mb (31.5% reduction)\n",
            "Bad transformation TransactionAmt\n",
            "Bad transformation dist1\n",
            "Bad transformation dist2\n",
            "Bad transformation C1\n",
            "Bad transformation C2\n",
            "Bad transformation C11\n",
            "Bad transformation C12\n",
            "Bad transformation D8\n",
            "Bad transformation D9\n",
            "Bad transformation V126\n",
            "Bad transformation V127\n",
            "Bad transformation V128\n",
            "Bad transformation V129\n",
            "Bad transformation V130\n",
            "Bad transformation V131\n",
            "Bad transformation V132\n",
            "Bad transformation V133\n",
            "Bad transformation V134\n",
            "Bad transformation V135\n",
            "Bad transformation V136\n",
            "Bad transformation V137\n",
            "Bad transformation V150\n",
            "Bad transformation V160\n",
            "Bad transformation V161\n",
            "Bad transformation V162\n",
            "Bad transformation V163\n",
            "Bad transformation V166\n",
            "Bad transformation V202\n",
            "Bad transformation V203\n",
            "Bad transformation V204\n",
            "Bad transformation V205\n",
            "Bad transformation V206\n",
            "Bad transformation V207\n",
            "Bad transformation V208\n",
            "Bad transformation V209\n",
            "Bad transformation V210\n",
            "Bad transformation V211\n",
            "Bad transformation V212\n",
            "Bad transformation V213\n",
            "Bad transformation V214\n",
            "Bad transformation V215\n",
            "Bad transformation V216\n",
            "Bad transformation V263\n",
            "Bad transformation V264\n",
            "Bad transformation V265\n",
            "Bad transformation V266\n",
            "Bad transformation V267\n",
            "Bad transformation V268\n",
            "Bad transformation V269\n",
            "Bad transformation V270\n",
            "Bad transformation V271\n",
            "Bad transformation V272\n",
            "Bad transformation V273\n",
            "Bad transformation V274\n",
            "Bad transformation V275\n",
            "Bad transformation V276\n",
            "Bad transformation V277\n",
            "Bad transformation V278\n",
            "Bad transformation V306\n",
            "Bad transformation V307\n",
            "Bad transformation V308\n",
            "Bad transformation V309\n",
            "Bad transformation V310\n",
            "Bad transformation V311\n",
            "Bad transformation V312\n",
            "Bad transformation V313\n",
            "Bad transformation V314\n",
            "Bad transformation V315\n",
            "Bad transformation V316\n",
            "Bad transformation V317\n",
            "Bad transformation V318\n",
            "Bad transformation V319\n",
            "Bad transformation V320\n",
            "Bad transformation V321\n",
            "Bad transformation V331\n",
            "Bad transformation V332\n",
            "Bad transformation V333\n",
            "Bad transformation V334\n",
            "Bad transformation V335\n",
            "Bad transformation V336\n",
            "Bad transformation V337\n",
            "Bad transformation V338\n",
            "Bad transformation V339\n",
            "Bad transformation DT\n",
            "Mem. usage decreased to 15.54 Mb (46.2% reduction)\n",
            "Bad transformation id_11\n",
            "Mem. usage decreased to 15.29 Mb (46.2% reduction)\n",
            "Bad transformation id_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ocWWqia4CT"
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "\n",
        "train_df.to_pickle('train_transaction.pkl')\n",
        "test_df.to_pickle('test_transaction.pkl')\n",
        "\n",
        "train_identity.to_pickle('train_identity.pkl')\n",
        "test_identity.to_pickle('test_identity.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX5sEbnKa9La",
        "outputId": "32557e2b-3d10-4b13-8570-4617ad053b1e"
      },
      "source": [
        "########################### Full minification for fast tests\n",
        "#################################################################################\n",
        "for df in [train_df, test_df, train_identity, test_identity]:\n",
        "    df = reduce_mem_usage(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 585.15 Mb (28.5% reduction)\n",
            "Mem. usage decreased to 509.80 Mb (27.5% reduction)\n",
            "Mem. usage decreased to 15.54 Mb (5.0% reduction)\n",
            "Mem. usage decreased to 15.29 Mb (5.0% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ppcns4wa9rQ"
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "\n",
        "train_df.to_pickle('train_transaction_mini.pkl')\n",
        "test_df.to_pickle('test_transaction_mini.pkl')\n",
        "\n",
        "train_identity.to_pickle('train_identity_mini.pkl')\n",
        "test_identity.to_pickle('test_identity_mini.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUE66i4Ra_2L"
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "import pickle\n",
        "possible_goups = [['V1', 'V2', 'V6', 'V7', 'V8', 'V9'],\n",
        " ['V1', 'V2', 'V3', 'V6', 'V7', 'V8', 'V9'],\n",
        " ['V2', 'V3', 'V6', 'V7', 'V8', 'V9'],\n",
        " ['V4', 'V5'],\n",
        " ['V10', 'V11'],\n",
        " ['V12', 'V13'],\n",
        " ['V14', 'V65'],\n",
        " ['V15', 'V16', 'V33', 'V34', 'V57', 'V58', 'V79', 'V94'],\n",
        " ['V15', 'V16', 'V33', 'V34', 'V57'],\n",
        " ['V17', 'V18', 'V21', 'V22'],\n",
        " ['V19', 'V20'],\n",
        " ['V17', 'V18', 'V21', 'V22', 'V63', 'V84'],\n",
        " ['V23', 'V24'],\n",
        " ['V25', 'V26'],\n",
        " ['V27', 'V28', 'V68', 'V89'],\n",
        " ['V29', 'V30', 'V69', 'V90', 'V91'],\n",
        " ['V29', 'V30', 'V70', 'V90', 'V91'],\n",
        " ['V31', 'V32', 'V50', 'V71', 'V92', 'V93'],\n",
        " ['V31', 'V32', 'V92'],\n",
        " ['V15', 'V16', 'V33', 'V34', 'V51', 'V94'],\n",
        " ['V15', 'V16', 'V33', 'V34'],\n",
        " ['V35', 'V36'],\n",
        " ['V37', 'V38'],\n",
        " ['V39', 'V40'],\n",
        " ['V41', 'V46', 'V47'],\n",
        " ['V42', 'V43', 'V84'],\n",
        " ['V42', 'V43'],\n",
        " ['V44', 'V45'],\n",
        " ['V48', 'V49'],\n",
        " ['V31', 'V50', 'V71', 'V92'],\n",
        " ['V33', 'V51', 'V52', 'V73', 'V94'],\n",
        " ['V51', 'V52'],\n",
        " ['V53', 'V54'],\n",
        " ['V15', 'V16', 'V57', 'V58', 'V73', 'V79', 'V94'],\n",
        " ['V15', 'V57', 'V58', 'V79'],\n",
        " ['V59', 'V60', 'V63'],\n",
        " ['V59', 'V60'],\n",
        " ['V61', 'V62'],\n",
        " ['V21', 'V59', 'V63', 'V64', 'V84'],\n",
        " ['V63', 'V64'],\n",
        " ['V66', 'V67'],\n",
        " ['V29', 'V69', 'V70', 'V90'],\n",
        " ['V30', 'V69', 'V70', 'V90', 'V91'],\n",
        " ['V31', 'V50', 'V71', 'V72', 'V92', 'V93'],\n",
        " ['V71', 'V72', 'V92', 'V93'],\n",
        " ['V51', 'V57', 'V73', 'V74', 'V94'],\n",
        " ['V73', 'V74'],\n",
        " ['V75', 'V76'],\n",
        " ['V80', 'V81', 'V84'],\n",
        " ['V80', 'V81'],\n",
        " ['V82', 'V83'],\n",
        " ['V21', 'V42', 'V63', 'V80', 'V84', 'V85'],\n",
        " ['V84', 'V85'],\n",
        " ['V86', 'V87'],\n",
        " ['V29', 'V30', 'V69', 'V70', 'V90', 'V91'],\n",
        " ['V31', 'V32', 'V50', 'V71', 'V72', 'V92', 'V93'],\n",
        " ['V31', 'V71', 'V72', 'V92', 'V93'],\n",
        " ['V15', 'V33', 'V51', 'V57', 'V73', 'V94'],\n",
        " ['V101',\n",
        "  'V102',\n",
        "  'V103',\n",
        "  'V143',\n",
        "  'V167',\n",
        "  'V168',\n",
        "  'V177',\n",
        "  'V178',\n",
        "  'V179',\n",
        "  'V279',\n",
        "  'V280',\n",
        "  'V293',\n",
        "  'V295',\n",
        "  'V322',\n",
        "  'V323',\n",
        "  'V324',\n",
        "  'V95',\n",
        "  'V96',\n",
        "  'V97'],\n",
        " ['V101',\n",
        "  'V102',\n",
        "  'V103',\n",
        "  'V143',\n",
        "  'V167',\n",
        "  'V168',\n",
        "  'V177',\n",
        "  'V178',\n",
        "  'V179',\n",
        "  'V279',\n",
        "  'V280',\n",
        "  'V293',\n",
        "  'V294',\n",
        "  'V295',\n",
        "  'V322',\n",
        "  'V323',\n",
        "  'V324',\n",
        "  'V95',\n",
        "  'V96',\n",
        "  'V97'],\n",
        " ['V105', 'V106', 'V296', 'V298', 'V299', 'V329', 'V330'],\n",
        " ['V105', 'V106', 'V298', 'V299', 'V329', 'V330'],\n",
        " ['V111', 'V113'],\n",
        " ['V126', 'V128', 'V132', 'V134'],\n",
        " ['V127', 'V128', 'V133', 'V134'],\n",
        " ['V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V332'],\n",
        " ['V129', 'V266', 'V269', 'V309', 'V334'],\n",
        " ['V130', 'V310'],\n",
        " ['V131', 'V312'],\n",
        " ['V126', 'V128', 'V132', 'V133', 'V134'],\n",
        " ['V127', 'V128', 'V132', 'V133', 'V134'],\n",
        " ['V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V318', 'V332'],\n",
        " ['V136', 'V137'],\n",
        " ['V101',\n",
        "  'V102',\n",
        "  'V103',\n",
        "  'V143',\n",
        "  'V167',\n",
        "  'V177',\n",
        "  'V178',\n",
        "  'V179',\n",
        "  'V279',\n",
        "  'V280',\n",
        "  'V293',\n",
        "  'V295',\n",
        "  'V322',\n",
        "  'V323',\n",
        "  'V324',\n",
        "  'V95',\n",
        "  'V96',\n",
        "  'V97'],\n",
        " ['V144', 'V145', 'V150', 'V151'],\n",
        " ['V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158'],\n",
        " ['V144', 'V145', 'V150', 'V151', 'V152'],\n",
        " ['V151', 'V152'],\n",
        " ['V161', 'V163'],\n",
        " ['V162', 'V163'],\n",
        " ['V161', 'V162', 'V163'],\n",
        " ['V101',\n",
        "  'V102',\n",
        "  'V103',\n",
        "  'V167',\n",
        "  'V168',\n",
        "  'V177',\n",
        "  'V178',\n",
        "  'V179',\n",
        "  'V279',\n",
        "  'V280',\n",
        "  'V293',\n",
        "  'V295',\n",
        "  'V322',\n",
        "  'V323',\n",
        "  'V324',\n",
        "  'V95',\n",
        "  'V96',\n",
        "  'V97'],\n",
        " ['V176', 'V190', 'V199', 'V228', 'V246', 'V257'],\n",
        " ['V180', 'V182', 'V183'],\n",
        " ['V181', 'V328'],\n",
        " ['V180', 'V182', 'V183', 'V330'],\n",
        " ['V186', 'V191', 'V196'],\n",
        " ['V187', 'V192'],\n",
        " ['V187', 'V192', 'V193'],\n",
        " ['V192', 'V193', 'V196'],\n",
        " ['V194', 'V197'],\n",
        " ['V195', 'V198'],\n",
        " ['V186', 'V191', 'V193', 'V196'],\n",
        " ['V202', 'V204', 'V211', 'V213'],\n",
        " ['V203', 'V204', 'V212'],\n",
        " ['V202', 'V203', 'V204', 'V213'],\n",
        " ['V202', 'V211', 'V213'],\n",
        " ['V203', 'V212', 'V213'],\n",
        " ['V202', 'V204', 'V211', 'V212', 'V213'],\n",
        " ['V214', 'V276', 'V337'],\n",
        " ['V215', 'V216', 'V277', 'V278', 'V338', 'V339'],\n",
        " ['V217', 'V219', 'V231', 'V233'],\n",
        " ['V218', 'V219', 'V232', 'V233'],\n",
        " ['V217', 'V218', 'V219', 'V231', 'V232', 'V233'],\n",
        " ['V222', 'V230'],\n",
        " ['V224', 'V225'],\n",
        " ['V229', 'V230', 'V258'],\n",
        " ['V222', 'V229', 'V230', 'V258'],\n",
        " ['V236', 'V237'],\n",
        " ['V238', 'V239'],\n",
        " ['V240', 'V241', 'V247', 'V252', 'V260'],\n",
        " ['V242', 'V244'],\n",
        " ['V245', 'V259'],\n",
        " ['V240', 'V241', 'V247', 'V249', 'V252'],\n",
        " ['V248', 'V249', 'V254'],\n",
        " ['V247', 'V248', 'V249', 'V252'],\n",
        " ['V250', 'V251'],\n",
        " ['V248', 'V254'],\n",
        " ['V255', 'V256'],\n",
        " ['V240', 'V241', 'V260'],\n",
        " ['V263', 'V265', 'V273', 'V274', 'V275'],\n",
        " ['V264', 'V265'],\n",
        " ['V263', 'V264', 'V265'],\n",
        " ['V129', 'V266', 'V269', 'V309', 'V334', 'V336'],\n",
        " ['V268', 'V336'],\n",
        " ['V270', 'V272'],\n",
        " ['V263', 'V273', 'V274', 'V275'],\n",
        " ['V291', 'V292'],\n",
        " ['V102', 'V280', 'V294', 'V295', 'V323', 'V96'],\n",
        " ['V105', 'V296', 'V298', 'V299', 'V329'],\n",
        " ['V105', 'V106', 'V296', 'V298', 'V299', 'V329'],\n",
        " ['V105', 'V106', 'V296', 'V298', 'V299', 'V330'],\n",
        " ['V300', 'V301'],\n",
        " ['V302', 'V304'],\n",
        " ['V303', 'V304'],\n",
        " ['V302', 'V303', 'V304'],\n",
        " ['V306', 'V308', 'V316', 'V318'],\n",
        " ['V307', 'V308', 'V317'],\n",
        " ['V306', 'V307', 'V308', 'V318'],\n",
        " ['V313', 'V315'],\n",
        " ['V306', 'V316', 'V318'],\n",
        " ['V307', 'V317', 'V318'],\n",
        " ['V134', 'V306', 'V308', 'V316', 'V317', 'V318'],\n",
        " ['V320', 'V321'],\n",
        " ['V326', 'V327'],\n",
        " ['V105', 'V106', 'V296', 'V298', 'V329', 'V330'],\n",
        " ['V105', 'V106', 'V183', 'V299', 'V329', 'V330'],\n",
        " ['V331', 'V332', 'V333'],\n",
        " ['V128', 'V134', 'V331', 'V332', 'V333'],\n",
        " ['V335', 'V336'],\n",
        " ['V266', 'V268', 'V269', 'V334', 'V335', 'V336']]\n",
        "\n",
        "with open('possible_goups.pickle', 'wb') as f:\n",
        "    pickle.dump(possible_goups, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgbGDUDwCqH"
      },
      "source": [
        "# IEEE - Basic FE - part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UddoMedIbEWJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, warnings, random, datetime, math\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, KFold,GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Ktc7cgNLK9"
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "## Global frequency encoding    \n",
        "def frequency_encoding(df, columns, self_encoding=False):\n",
        "    for col in columns:\n",
        "        fq_encode = df[col].value_counts(dropna=False).to_dict()\n",
        "        if self_encoding:\n",
        "            df[col] = df[col].map(fq_encode)\n",
        "        else:\n",
        "            df[col+'_fq_enc'] = df[col].map(fq_encode)\n",
        "    return df\n",
        "\n",
        "\n",
        "def values_normalization(dt_df, periods, columns, enc_type='both'):\n",
        "    for period in periods:\n",
        "        for col in columns:\n",
        "            new_col = col +'_'+ period\n",
        "            dt_df[col] = dt_df[col].astype(float)  \n",
        "\n",
        "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
        "            temp_min.index = temp_min[period].values\n",
        "            temp_min = temp_min['min'].to_dict()\n",
        "\n",
        "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
        "            temp_max.index = temp_max[period].values\n",
        "            temp_max = temp_max['max'].to_dict()\n",
        "\n",
        "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
        "            temp_mean.index = temp_mean[period].values\n",
        "            temp_mean = temp_mean['mean'].to_dict()\n",
        "\n",
        "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
        "            temp_std.index = temp_std[period].values\n",
        "            temp_std = temp_std['std'].to_dict()\n",
        "\n",
        "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
        "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
        "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
        "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
        "            \n",
        "            if enc_type=='both':\n",
        "                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
        "                dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
        "            elif enc_type=='norm':\n",
        "                 dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
        "            elif enc_type=='min_max':\n",
        "                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
        "\n",
        "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
        "    return dt_df\n",
        "\n",
        "def get_new_columns(temp_list):\n",
        "    temp_list = [col for col in list(full_df) if col not in temp_list]\n",
        "    temp_list.sort()\n",
        "\n",
        "    temp_list2 = [col if col not in remove_features else '-' for col in temp_list ]\n",
        "    temp_list2.sort()\n",
        "\n",
        "    temp_list = {'New columns (including dummy)': temp_list,\n",
        "                 'New Features': temp_list2}\n",
        "    temp_list = pd.DataFrame.from_dict(temp_list)\n",
        "    return temp_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMchpjn3NNnk"
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "SEED = 42\n",
        "seed_everything(SEED)\n",
        "LOCAL_TEST = True\n",
        "MAKE_TESTS = True\n",
        "TARGET = 'isFraud'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPsivN7ENPte"
      },
      "source": [
        "########################### Model params\n",
        "lgb_params = {\n",
        "                    'objective':'binary',\n",
        "                    'boosting_type':'gbdt',\n",
        "                    'metric':'auc',\n",
        "                    'n_jobs':-1,\n",
        "                    'learning_rate':0.01,\n",
        "                    'num_leaves': 2**8,\n",
        "                    'max_depth':-1,\n",
        "                    'tree_learner':'serial',\n",
        "                    'colsample_bytree': 0.7,\n",
        "                    'subsample_freq':1,\n",
        "                    'subsample':0.7,\n",
        "                    'n_estimators':80000,\n",
        "                    'max_bin':255,\n",
        "                    'verbose':-1,\n",
        "                    'seed': SEED,\n",
        "                    'early_stopping_rounds':100, \n",
        "                } "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HyleP6NRYi"
      },
      "source": [
        "########################### Model\n",
        "import lightgbm as lgb\n",
        "\n",
        "def make_test(old_score=0, output=False):\n",
        "\n",
        "    features_columns = [col for col in list(full_df) if col not in remove_features]\n",
        "    train_mask = full_df['TransactionID'].isin(local_train_id['TransactionID'])\n",
        "    test_mask = full_df['TransactionID'].isin(local_test_id['TransactionID'])\n",
        "    \n",
        "    X,y = full_df[train_mask][features_columns], full_df[train_mask][TARGET]    \n",
        "    P,P_y = full_df[test_mask][features_columns], full_df[test_mask][TARGET]  \n",
        "\n",
        "    for col in list(X):\n",
        "        if X[col].dtype=='O':\n",
        "            X[col] = X[col].fillna('unseen_before_label')\n",
        "            P[col] = P[col].fillna('unseen_before_label')\n",
        "\n",
        "            X[col] = X[col].astype(str)\n",
        "            P[col] = P[col].astype(str)\n",
        "\n",
        "            le = LabelEncoder()\n",
        "            le.fit(list(X[col])+list(P[col]))\n",
        "            X[col] = le.transform(X[col])\n",
        "            P[col]  = le.transform(P[col])\n",
        "\n",
        "            X[col] = X[col].astype('category')\n",
        "            P[col] = P[col].astype('category')\n",
        "        \n",
        "    tt_df = full_df[test_mask][['TransactionID','DT_W',TARGET]]        \n",
        "    tt_df['prediction'] = 0\n",
        "    \n",
        "    tr_data = lgb.Dataset(X, label=y)\n",
        "    vl_data = lgb.Dataset(P, label=P_y) \n",
        "    estimator = lgb.train(\n",
        "            lgb_params,\n",
        "            tr_data,\n",
        "            valid_sets = [tr_data, vl_data],\n",
        "            verbose_eval = 200,\n",
        "        )   \n",
        "        \n",
        "    tt_df['prediction'] = estimator.predict(P)\n",
        "    feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
        "    \n",
        "    if output:\n",
        "        tt_df[['TransactionID','prediction']].to_csv('oof.csv',index=False)\n",
        "        print('---Wrote OOF to file---')\n",
        "    \n",
        "    m_results = []\n",
        "    print('#'*20)\n",
        "    g_auc = metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction'])\n",
        "    score_diff = g_auc - old_score\n",
        "    print('Global AUC', g_auc)\n",
        "    m_results.append(g_auc)\n",
        "    \n",
        "    for i in range(full_df[test_mask]['DT_W'].min(), full_df[test_mask]['DT_W'].max()+1):\n",
        "        mask = tt_df['DT_W']==i\n",
        "        w_auc = metrics.roc_auc_score(tt_df[mask][TARGET], tt_df[mask]['prediction'])\n",
        "        print('Week', i, w_auc, len(tt_df[mask]))\n",
        "        m_results.append(w_auc)\n",
        "        \n",
        "    print('#'*20)\n",
        "    print('Features Preformance:', g_auc)\n",
        "    print('Diff with previous__:', score_diff)\n",
        "    \n",
        "    return tt_df, feature_imp, m_results, estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVRlDMtRNTiL",
        "outputId": "d05bee9a-4d76-4093-e0dd-a721b3dd2540"
      },
      "source": [
        "########################### DATA LOAD\n",
        "#################################################################################\n",
        "print('Load Data')\n",
        "train_df = pd.read_pickle('train_transaction.pkl')\n",
        "test_df = pd.read_pickle('test_transaction.pkl')\n",
        "\n",
        "# Full Data set (careful with target encoding)\n",
        "full_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
        "\n",
        "# Local test IDs with one month gap\n",
        "local_test_id  = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n",
        "local_train_id = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n",
        "local_train_id = local_train_id[['TransactionID']]\n",
        "local_test_id  = local_test_id[['TransactionID']]\n",
        "del train_df, test_df\n",
        "\n",
        "# Identity Data set\n",
        "train_identity = pd.read_pickle('train_identity.pkl')\n",
        "test_identity = pd.read_pickle('test_identity.pkl')\n",
        "identity_df = pd.concat([train_identity, test_identity]).reset_index(drop=True)\n",
        "del train_identity, test_identity\n",
        "\n",
        "print('Shape control (for local test):', local_train_id.shape, local_test_id.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load Data\n",
            "Shape control (for local test): (417559, 1) (89326, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c_I8Tq8NlCX"
      },
      "source": [
        "########################### All features columns\n",
        "#################################################################################\n",
        "# Add list of feature that we will remove for sure\n",
        "remove_features = [\n",
        "    'TransactionID','TransactionDT', \n",
        "    TARGET,\n",
        "    'DT','DT_M','DT_W','DT_D','DTT',\n",
        "    'DT_hour','DT_day_week','DT_day_month',\n",
        "    'DT_M_total','DT_W_total','DT_D_total',\n",
        "    'is_december','is_holiday','temp','weight',\n",
        "    ]\n",
        "\n",
        "# Make sure that TransactionAmt is float64\n",
        "# To not lose values during aggregations\n",
        "full_df['TransactionAmt'] = full_df['TransactionAmt'].astype(float)\n",
        "\n",
        "# Base lists for features to do frequency encoding\n",
        "# and saved initial state\n",
        "fq_encode = []\n",
        "base_columns = list(full_df)\n",
        "\n",
        "# We don't need V columns in the initial phase \n",
        "# removing them to make predictions faster\n",
        "remove_features += ['V'+str(i) for i in range(1,340)]\n",
        "\n",
        "# Removing transformed D columns\n",
        "remove_features += ['uid_td_D'+str(i) for i in range(1,16) if i!=9]\n",
        "\n",
        "# Make sure we have m_results variable\n",
        "m_results = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pcR-cF6NqJT",
        "outputId": "8a4777c4-3df9-4339-cd81-f9fa696e35c2"
      },
      "source": [
        "########################### This is start baseline\n",
        "if MAKE_TESTS:\n",
        "    tt_df, feature_imp, m_results, model = make_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.955715\tvalid_1's auc: 0.896712\n",
            "[400]\ttraining's auc: 0.982231\tvalid_1's auc: 0.909409\n",
            "[600]\ttraining's auc: 0.991262\tvalid_1's auc: 0.912886\n",
            "[800]\ttraining's auc: 0.995355\tvalid_1's auc: 0.914254\n",
            "Early stopping, best iteration is:\n",
            "[830]\ttraining's auc: 0.995736\tvalid_1's auc: 0.914355\n",
            "####################\n",
            "Global AUC 0.9143548683934609\n",
            "Week 70 0.9230980050488626 18970\n",
            "Week 71 0.9067099636764909 20726\n",
            "Week 72 0.9051352197870371 20332\n",
            "Week 73 0.9249483056957462 19010\n",
            "Week 74 0.9078872187307547 10288\n",
            "####################\n",
            "Features Preformance: 0.9143548683934609\n",
            "Diff with previous__: 0.9143548683934609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAPdmilNqny",
        "outputId": "41312ce1-eecd-484e-e458-8f65e19a487c"
      },
      "source": [
        "########################### Fix card columns and encode\n",
        "print('Fix card4 and card6 values')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "####\n",
        "# card4 and card5 have strong connection\n",
        "# with card1 - we can unify values\n",
        "# to guarantee that it will be same combinations\n",
        "# for all data.\n",
        "\n",
        "# I've tried to fill others NaNs\n",
        "# But seems that there are no more bad values.\n",
        "# All rest NaNs are meaningful.\n",
        "####\n",
        "\n",
        "full_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\n",
        "full_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n",
        "\n",
        "i_cols = ['card4','card6']\n",
        "\n",
        "for col in i_cols:\n",
        "    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n",
        "    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False)\n",
        "    del temp_df['count']\n",
        "    temp_df = temp_df.drop_duplicates(subset=['card1'], keep='first').reset_index(drop=True)\n",
        "    temp_df.index = temp_df['card1'].values\n",
        "    temp_df = temp_df[col].to_dict()\n",
        "    full_df[col] = full_df['card1'].map(temp_df)\n",
        "    \n",
        "# Add cards features for later encoding\n",
        "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
        "fq_encode += i_cols\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fix card4 and card6 values\n",
            "Empty DataFrame\n",
            "Columns: [New columns (including dummy), New Features]\n",
            "Index: []\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.955504\tvalid_1's auc: 0.897144\n",
            "[400]\ttraining's auc: 0.98198\tvalid_1's auc: 0.910281\n",
            "[600]\ttraining's auc: 0.991192\tvalid_1's auc: 0.913685\n",
            "[800]\ttraining's auc: 0.995295\tvalid_1's auc: 0.914555\n",
            "Early stopping, best iteration is:\n",
            "[865]\ttraining's auc: 0.996118\tvalid_1's auc: 0.914708\n",
            "####################\n",
            "Global AUC 0.914707567231095\n",
            "Week 70 0.9236173561571233 18970\n",
            "Week 71 0.9067164773859334 20726\n",
            "Week 72 0.9055725078756296 20332\n",
            "Week 73 0.9252666186012978 19010\n",
            "Week 74 0.9088326006873009 10288\n",
            "####################\n",
            "Features Preformance: 0.914707567231095\n",
            "Diff with previous__: 0.00035269883763411336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHz4LOKmNsLT",
        "outputId": "1806aadd-19fb-484d-cb8b-432f3f5d1725"
      },
      "source": [
        "########################### Client Virtual ID\n",
        "print('Create client identification ID')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "####\n",
        "# Client subgroups:\n",
        "\n",
        "# bank_type -> looking on card3 and card5 distributions\n",
        "# I would say it is bank branch and country\n",
        "# full_addr -> Client registration address in bank\n",
        "# uid1 -> client identification by bank and card type\n",
        "# uid2 -> client identification with additional geo information\n",
        "####\n",
        "\n",
        "# Bank type\n",
        "full_df['bank_type'] = full_df['card3'].astype(str)+'_'+full_df['card5'].astype(str)\n",
        "\n",
        "# Full address\n",
        "full_df['full_addr'] = full_df['addr1'].astype(str)+'_'+full_df['addr2'].astype(str)\n",
        "\n",
        "# Virtual client uid\n",
        "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
        "full_df['uid1'] = ''\n",
        "for col in i_cols:\n",
        "    full_df['uid1'] += full_df[col].astype(str)+'_'\n",
        "\n",
        "# Virtual client uid + full_addr\n",
        "full_df['uid2'] = full_df['uid1']+'_'+full_df['full_addr'].astype(str)\n",
        "\n",
        "\n",
        "# Add uids features for later encoding\n",
        "i_cols = ['full_addr','bank_type','uid1','uid2']\n",
        "fq_encode += i_cols\n",
        "\n",
        "# We can't use this features directly because\n",
        "# test data will have many unknow values\n",
        "remove_features += i_cols\n",
        "\n",
        "# We've created just \"ghost\" features -> no need to run test\n",
        "if False: \n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create client identification ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTH1S2jANt_3",
        "outputId": "1484d64d-fed5-406f-ae13-13b5e9f4945e"
      },
      "source": [
        "########################### Client identification using deltas\n",
        "print('Create client identification ID using deltas')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "# Temporary list\n",
        "client_cols = []\n",
        "\n",
        "# Convert all delta columns to some date\n",
        "# D8 and D9 are not days deltas -\n",
        "# we can try convert D8 to int and \n",
        "# probably it will give us date\n",
        "# but I'm very very unsure about it.\n",
        "\n",
        "# We will do all D columns transformation\n",
        "# (but save original values) as we will\n",
        "# use it later for other features.\n",
        "\n",
        "for col in ['D'+str(i) for i in range(1,16) if i!=9]: \n",
        "    new_col = 'uid_td_'+str(col)\n",
        "    \n",
        "    new_col = 'uid_td_'+str(col)\n",
        "    full_df[new_col] = full_df['TransactionDT'] / (24*60*60)\n",
        "    full_df[new_col] = np.floor(full_df[new_col] - full_df[col])    \n",
        "    remove_features.append(new_col)\n",
        "    \n",
        "    # Date is useless itself -> add to dummy features\n",
        "    #remove_features.append(new_col)\n",
        "\n",
        "\n",
        "# The most possible deltas to identify account or client\n",
        "# initial activity are 'D1','D10','D15'\n",
        "# We can try to find certain client using uid and date\n",
        "# If client is the same uid+date combination will be\n",
        "# unique per client and all his transactions\n",
        "for col in ['D1','D10','D15']:\n",
        "    new_col = 'uid_td_'+str(col)\n",
        "\n",
        "    # card1 + full_addr + date\n",
        "    full_df[new_col+'_cUID_1'] = full_df['card1'].astype(str)+'_'+full_df['full_addr'].astype(str)+'_'+full_df[new_col].astype(str)\n",
        "    \n",
        "    # uid1 + full_addr + date\n",
        "    full_df[new_col+'_cUID_2'] = full_df['uid2'].astype(str)+'_'+full_df[new_col].astype(str)\n",
        "\n",
        "    # columns 'D1','D2' are clipped we can't trust maximum values\n",
        "    if col in ['D1','D2']:\n",
        "        full_df[new_col+'_cUID_1'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_1'])\n",
        "        full_df[new_col+'_cUID_2'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_2'])\n",
        "\n",
        "    full_df[new_col+'_cUID_1'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_1'])\n",
        "    full_df[new_col+'_cUID_2'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_2'])\n",
        "\n",
        "    # reset cUID_1 if both address are nan (very unstable prediction)\n",
        "    full_df[new_col+'_cUID_1'] = np.where(full_df['addr1'].isna()&full_df['addr2'].isna(), np.nan, full_df[new_col+'_cUID_1'])\n",
        "\n",
        "    # cUID is useless itself -> add to dummy features\n",
        "    remove_features += [new_col+'_cUID_1',new_col+'_cUID_2']\n",
        "    \n",
        "    # Add to temporary list (to join with encoding list later)\n",
        "    client_cols += [new_col+'_cUID_1',new_col+'_cUID_2']\n",
        "    \n",
        "## Best candidate for client complete identification\n",
        "## uid_td_D1_cUID_1\n",
        "        \n",
        "# Add cUIDs features for later encoding\n",
        "fq_encode += client_cols\n",
        "\n",
        "# We will save this list and even append \n",
        "# few more columns for later use\n",
        "client_cols += ['card1','card2','card3','card4','card5',\n",
        "                'uid1','uid2']\n",
        "\n",
        "####\n",
        "# We've created just \"ghost\" features -> no need to run test\n",
        "if False: \n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create client identification ID using deltas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQcJrX2wNvx2",
        "outputId": "56ff4c0e-106f-48da-b577-b269f2ae6aa5"
      },
      "source": [
        "########################### Mark card columns \"outliers\"\n",
        "print('Outliers mark')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "####\n",
        "# We are checking card and uid activity -\n",
        "# weither activity is constant during the year\n",
        "# or we have just single card/account use cases.\n",
        "\n",
        "# These features are categorical ones and\n",
        "# Catboost benefits the most from them.\n",
        "\n",
        "# Strange things:\n",
        "# - \"Time window\" should be big enough \n",
        "# - Doesn't work for DT_W and DT_D\n",
        "# even when local test showing score boost.\n",
        "\n",
        "# Seems to me that catboost start to combine \n",
        "# them with themselfs and loosing \"magic\".\n",
        "####\n",
        "\n",
        "i_cols = client_cols.copy()\n",
        "periods = ['DT_M'] \n",
        "\n",
        "for period in periods:\n",
        "    for col in i_cols:\n",
        "        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n",
        "        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0)\n",
        "        \n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers mark\n",
            "            New columns (including dummy)                           New Features\n",
            "0               card1_catboost_check_DT_M              card1_catboost_check_DT_M\n",
            "1               card2_catboost_check_DT_M              card2_catboost_check_DT_M\n",
            "2               card3_catboost_check_DT_M              card3_catboost_check_DT_M\n",
            "3               card4_catboost_check_DT_M              card4_catboost_check_DT_M\n",
            "4               card5_catboost_check_DT_M              card5_catboost_check_DT_M\n",
            "5                uid1_catboost_check_DT_M               uid1_catboost_check_DT_M\n",
            "6                uid2_catboost_check_DT_M               uid2_catboost_check_DT_M\n",
            "7   uid_td_D10_cUID_1_catboost_check_DT_M  uid_td_D10_cUID_1_catboost_check_DT_M\n",
            "8   uid_td_D10_cUID_2_catboost_check_DT_M  uid_td_D10_cUID_2_catboost_check_DT_M\n",
            "9   uid_td_D15_cUID_1_catboost_check_DT_M  uid_td_D15_cUID_1_catboost_check_DT_M\n",
            "10  uid_td_D15_cUID_2_catboost_check_DT_M  uid_td_D15_cUID_2_catboost_check_DT_M\n",
            "11   uid_td_D1_cUID_1_catboost_check_DT_M   uid_td_D1_cUID_1_catboost_check_DT_M\n",
            "12   uid_td_D1_cUID_2_catboost_check_DT_M   uid_td_D1_cUID_2_catboost_check_DT_M\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.956362\tvalid_1's auc: 0.897402\n",
            "[400]\ttraining's auc: 0.982985\tvalid_1's auc: 0.910812\n",
            "[600]\ttraining's auc: 0.99176\tvalid_1's auc: 0.914298\n",
            "[800]\ttraining's auc: 0.995676\tvalid_1's auc: 0.915326\n",
            "[1000]\ttraining's auc: 0.997654\tvalid_1's auc: 0.91538\n",
            "Early stopping, best iteration is:\n",
            "[903]\ttraining's auc: 0.996852\tvalid_1's auc: 0.915554\n",
            "####################\n",
            "Global AUC 0.9155541904571786\n",
            "Week 70 0.921344657766495 18970\n",
            "Week 71 0.9088296013609822 20726\n",
            "Week 72 0.9092740271139946 20332\n",
            "Week 73 0.9252289834174476 19010\n",
            "Week 74 0.9088958093646279 10288\n",
            "####################\n",
            "Features Preformance: 0.9155541904571786\n",
            "Diff with previous__: 0.0008466232260835316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P86FGbUWNxMq",
        "outputId": "5082323b-0522-4041-dc73-7cb0a70afc84"
      },
      "source": [
        "########################### V columns compact and assign groups\n",
        "print('V columns / Nan groups')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "####\n",
        "# Nangroups identification are categorical features\n",
        "# and Catboost benefits the most from them.\n",
        "\n",
        "# Mean/std just occasion transformation.\n",
        "####\n",
        "\n",
        "nans_groups = {}\n",
        "nans_df = full_df.isna()\n",
        "\n",
        "i_cols = ['V'+str(i) for i in range(1,340)]\n",
        "for col in i_cols:\n",
        "    cur_group = nans_df[col].sum()\n",
        "    try:\n",
        "        nans_groups[cur_group].append(col)\n",
        "    except:\n",
        "        nans_groups[cur_group]=[col]\n",
        "\n",
        "for col in nans_groups:\n",
        "    # Very doubtful features -> Seems it works in tandem with other feature\n",
        "    # But I'm not sure\n",
        "    full_df['nan_group_sum_'+str(col)] = full_df[nans_groups[col]].to_numpy().sum(axis=1)\n",
        "    full_df['nan_group_mean_'+str(col)] = full_df[nans_groups[col]].to_numpy().mean(axis=1)\n",
        "        \n",
        "    # lgbm doesn't benefit from such feature -> \n",
        "    # let's transform and add it to dummy features list\n",
        "    full_df['nan_group_catboost_'+str(col)]  = np.where(nans_df[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n",
        "    remove_features.append('nan_group_catboost_'+str(col))\n",
        "        \n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V columns / Nan groups\n",
            "   New columns (including dummy)           New Features\n",
            "0      nan_group_catboost_101245                      -\n",
            "1          nan_group_catboost_15                      -\n",
            "2      nan_group_catboost_245823                      -\n",
            "3         nan_group_catboost_314                      -\n",
            "4      nan_group_catboost_455805                      -\n",
            "5        nan_group_catboost_7300                      -\n",
            "6      nan_group_catboost_818499                      -\n",
            "7      nan_group_catboost_820866                      -\n",
            "8      nan_group_catboost_821037                      -\n",
            "9      nan_group_catboost_840073                      -\n",
            "10      nan_group_catboost_88662                      -\n",
            "11      nan_group_catboost_89995                      -\n",
            "12     nan_group_catboost_938449                      -\n",
            "13     nan_group_catboost_939225                      -\n",
            "14     nan_group_catboost_939501                      -\n",
            "15         nan_group_mean_101245  nan_group_mean_101245\n",
            "16             nan_group_mean_15      nan_group_mean_15\n",
            "17         nan_group_mean_245823  nan_group_mean_245823\n",
            "18            nan_group_mean_314     nan_group_mean_314\n",
            "19         nan_group_mean_455805  nan_group_mean_455805\n",
            "20           nan_group_mean_7300    nan_group_mean_7300\n",
            "21         nan_group_mean_818499  nan_group_mean_818499\n",
            "22         nan_group_mean_820866  nan_group_mean_820866\n",
            "23         nan_group_mean_821037  nan_group_mean_821037\n",
            "24         nan_group_mean_840073  nan_group_mean_840073\n",
            "25          nan_group_mean_88662   nan_group_mean_88662\n",
            "26          nan_group_mean_89995   nan_group_mean_89995\n",
            "27         nan_group_mean_938449  nan_group_mean_938449\n",
            "28         nan_group_mean_939225  nan_group_mean_939225\n",
            "29         nan_group_mean_939501  nan_group_mean_939501\n",
            "30          nan_group_sum_101245   nan_group_sum_101245\n",
            "31              nan_group_sum_15       nan_group_sum_15\n",
            "32          nan_group_sum_245823   nan_group_sum_245823\n",
            "33             nan_group_sum_314      nan_group_sum_314\n",
            "34          nan_group_sum_455805   nan_group_sum_455805\n",
            "35            nan_group_sum_7300     nan_group_sum_7300\n",
            "36          nan_group_sum_818499   nan_group_sum_818499\n",
            "37          nan_group_sum_820866   nan_group_sum_820866\n",
            "38          nan_group_sum_821037   nan_group_sum_821037\n",
            "39          nan_group_sum_840073   nan_group_sum_840073\n",
            "40           nan_group_sum_88662    nan_group_sum_88662\n",
            "41           nan_group_sum_89995    nan_group_sum_89995\n",
            "42          nan_group_sum_938449   nan_group_sum_938449\n",
            "43          nan_group_sum_939225   nan_group_sum_939225\n",
            "44          nan_group_sum_939501   nan_group_sum_939501\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.956839\tvalid_1's auc: 0.89452\n",
            "[400]\ttraining's auc: 0.983598\tvalid_1's auc: 0.909812\n",
            "[600]\ttraining's auc: 0.992497\tvalid_1's auc: 0.9138\n",
            "[800]\ttraining's auc: 0.996235\tvalid_1's auc: 0.915096\n",
            "[1000]\ttraining's auc: 0.997988\tvalid_1's auc: 0.915381\n",
            "Early stopping, best iteration is:\n",
            "[979]\ttraining's auc: 0.997838\tvalid_1's auc: 0.915525\n",
            "####################\n",
            "Global AUC 0.9155245403177976\n",
            "Week 70 0.921276725370722 18970\n",
            "Week 71 0.9074205710607384 20726\n",
            "Week 72 0.9076735527097464 20332\n",
            "Week 73 0.9273162941600576 19010\n",
            "Week 74 0.9097569963636274 10288\n",
            "####################\n",
            "Features Preformance: 0.9155245403177976\n",
            "Diff with previous__: -2.9650139380943408e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwB4jt9BNy35",
        "outputId": "c2b47303-f1f2-4f37-c3c6-e4d04d397b3d"
      },
      "source": [
        "########################### Mean encoding using M columns\n",
        "print('Mean encoding, using M columns')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "main_cols = {\n",
        "             'uid_td_D1_cUID_1':   ['M'+str(i) for i in [2,3,5,7,8,9]],\n",
        "             'uid_td_D1_cUID_2':   ['M'+str(i) for i in [2,3,5,6,9]],\n",
        "             'uid_td_D10_cUID_1':  ['M'+str(i) for i in [5,7,8,9]],\n",
        "             'uid_td_D10_cUID_2':  ['M'+str(i) for i in [3,6,7,8]],\n",
        "             'uid_td_D15_cUID_1':  ['M'+str(i) for i in [2,3,5,6,8,]],\n",
        "             'uid_td_D15_cUID_2':  ['M'+str(i) for i in [2,3,5,6,7,8]],\n",
        "             'card1':  ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n",
        "             'card2':  ['M'+str(i) for i in [1,2,3,7,9]],\n",
        "             'card4':  ['M'+str(i) for i in [3,7,8]],\n",
        "             'card5':  ['M'+str(i) for i in [5,6,8]],\n",
        "             'uid1':   ['M'+str(i) for i in [3,5,6,7,8,9]],\n",
        "             'uid2':   ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n",
        "            }\n",
        "\n",
        "for main_col,i_cols in main_cols.items():\n",
        "    for agg_type in ['mean']:\n",
        "        temp_df = full_df[[main_col]+i_cols]\n",
        "        temp_df = temp_df.groupby([main_col])[i_cols].transform(agg_type)\n",
        "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
        "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
        "        \n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean encoding, using M columns\n",
            "   New columns (including dummy)              New Features\n",
            "0                  card1_M2_mean             card1_M2_mean\n",
            "1                  card1_M3_mean             card1_M3_mean\n",
            "2                  card1_M5_mean             card1_M5_mean\n",
            "3                  card1_M6_mean             card1_M6_mean\n",
            "4                  card1_M7_mean             card1_M7_mean\n",
            "..                           ...                       ...\n",
            "56      uid_td_D1_cUID_2_M2_mean  uid_td_D1_cUID_2_M2_mean\n",
            "57      uid_td_D1_cUID_2_M3_mean  uid_td_D1_cUID_2_M3_mean\n",
            "58      uid_td_D1_cUID_2_M5_mean  uid_td_D1_cUID_2_M5_mean\n",
            "59      uid_td_D1_cUID_2_M6_mean  uid_td_D1_cUID_2_M6_mean\n",
            "60      uid_td_D1_cUID_2_M9_mean  uid_td_D1_cUID_2_M9_mean\n",
            "\n",
            "[61 rows x 2 columns]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.971478\tvalid_1's auc: 0.907757\n",
            "[400]\ttraining's auc: 0.991866\tvalid_1's auc: 0.922012\n",
            "[600]\ttraining's auc: 0.996987\tvalid_1's auc: 0.92657\n",
            "[800]\ttraining's auc: 0.998606\tvalid_1's auc: 0.927374\n",
            "Early stopping, best iteration is:\n",
            "[827]\ttraining's auc: 0.998741\tvalid_1's auc: 0.927506\n",
            "####################\n",
            "Global AUC 0.9275060983184914\n",
            "Week 70 0.9283000193612001 18970\n",
            "Week 71 0.9202475209588181 20726\n",
            "Week 72 0.9166413004023406 20332\n",
            "Week 73 0.942095674116799 19010\n",
            "Week 74 0.9280757704650585 10288\n",
            "####################\n",
            "Features Preformance: 0.9275060983184914\n",
            "Diff with previous__: 0.011981558000693782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClXG_UhVN0hu",
        "outputId": "f9415dec-2b72-45c0-ac59-b50a51f00904"
      },
      "source": [
        "########################### D Columns Mean/Std\n",
        "print('D columns Mean/Std')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "i_cols = ['D'+str(i) for i in range(1,16)]\n",
        "main_cols = {\n",
        "             'uid_td_D1_cUID_1': ['D'+str(i) for i in [1,2,3,10,11,14,15]],\n",
        "            }\n",
        "\n",
        "for main_col,i_cols in main_cols.items():\n",
        "    print(main_col)\n",
        "    for agg_type in ['mean','std']:\n",
        "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
        "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
        "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
        "        \n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D columns Mean/Std\n",
            "uid_td_D1_cUID_1\n",
            "   New columns (including dummy)               New Features\n",
            "0      uid_td_D1_cUID_1_D10_mean  uid_td_D1_cUID_1_D10_mean\n",
            "1       uid_td_D1_cUID_1_D10_std   uid_td_D1_cUID_1_D10_std\n",
            "2      uid_td_D1_cUID_1_D11_mean  uid_td_D1_cUID_1_D11_mean\n",
            "3       uid_td_D1_cUID_1_D11_std   uid_td_D1_cUID_1_D11_std\n",
            "4      uid_td_D1_cUID_1_D14_mean  uid_td_D1_cUID_1_D14_mean\n",
            "5       uid_td_D1_cUID_1_D14_std   uid_td_D1_cUID_1_D14_std\n",
            "6      uid_td_D1_cUID_1_D15_mean  uid_td_D1_cUID_1_D15_mean\n",
            "7       uid_td_D1_cUID_1_D15_std   uid_td_D1_cUID_1_D15_std\n",
            "8       uid_td_D1_cUID_1_D1_mean   uid_td_D1_cUID_1_D1_mean\n",
            "9        uid_td_D1_cUID_1_D1_std    uid_td_D1_cUID_1_D1_std\n",
            "10      uid_td_D1_cUID_1_D2_mean   uid_td_D1_cUID_1_D2_mean\n",
            "11       uid_td_D1_cUID_1_D2_std    uid_td_D1_cUID_1_D2_std\n",
            "12      uid_td_D1_cUID_1_D3_mean   uid_td_D1_cUID_1_D3_mean\n",
            "13       uid_td_D1_cUID_1_D3_std    uid_td_D1_cUID_1_D3_std\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.979602\tvalid_1's auc: 0.923059\n",
            "[400]\ttraining's auc: 0.993989\tvalid_1's auc: 0.93217\n",
            "[600]\ttraining's auc: 0.997809\tvalid_1's auc: 0.93474\n",
            "Early stopping, best iteration is:\n",
            "[670]\ttraining's auc: 0.998371\tvalid_1's auc: 0.93513\n",
            "####################\n",
            "Global AUC 0.9351298531579082\n",
            "Week 70 0.9400131715962973 18970\n",
            "Week 71 0.9281529418977119 20726\n",
            "Week 72 0.9281048609726068 20332\n",
            "Week 73 0.9460591204037491 19010\n",
            "Week 74 0.9300309922388237 10288\n",
            "####################\n",
            "Features Preformance: 0.9351298531579082\n",
            "Diff with previous__: 0.0076237548394167964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AuRfFUDN2VX",
        "outputId": "d6e838f5-0720-4e51-e45c-abe92c15c90f"
      },
      "source": [
        "########################### TransactionAmt\n",
        "print('TransactionAmt normalization')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "# Decimal part\n",
        "full_df['TransactionAmt_cents'] = np.round(100.*(full_df['TransactionAmt'] - np.floor(full_df['TransactionAmt'])),0)\n",
        "full_df['TransactionAmt_cents'] = full_df['TransactionAmt_cents'].astype(np.int8)\n",
        "\n",
        "# Clip top values\n",
        "full_df['TransactionAmt'] = full_df['TransactionAmt'].clip(0,5000)\n",
        "\n",
        "# Normalization by product\n",
        "main_cols = [\n",
        "             'uid_td_D1_cUID_1','uid_td_D1_cUID_2',\n",
        "             'uid_td_D10_cUID_1','uid_td_D10_cUID_2',\n",
        "             'uid_td_D15_cUID_1','uid_td_D15_cUID_2',\n",
        "             'card1','card3',\n",
        "            ]\n",
        "\n",
        "for col in main_cols:\n",
        "    for agg_type in ['mean','std']:\n",
        "        full_df[col+'_TransactionAmt_Product_' + agg_type] =\\\n",
        "                full_df.groupby([col,'ProductCD'])['TransactionAmt'].transform(agg_type)\n",
        "\n",
        "    f_std = col+'_TransactionAmt_Product_std'\n",
        "    f_mean = col+'_TransactionAmt_Product_mean'\n",
        "    full_df[col+'_Product_norm'] = (full_df['TransactionAmt']-full_df[f_mean])/full_df[f_std]\n",
        "    del full_df[f_mean], full_df[f_std]\n",
        "    \n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransactionAmt normalization\n",
            "    New columns (including dummy)                    New Features\n",
            "0            TransactionAmt_cents            TransactionAmt_cents\n",
            "1              card1_Product_norm              card1_Product_norm\n",
            "2              card3_Product_norm              card3_Product_norm\n",
            "3  uid_td_D10_cUID_1_Product_norm  uid_td_D10_cUID_1_Product_norm\n",
            "4  uid_td_D10_cUID_2_Product_norm  uid_td_D10_cUID_2_Product_norm\n",
            "5  uid_td_D15_cUID_1_Product_norm  uid_td_D15_cUID_1_Product_norm\n",
            "6  uid_td_D15_cUID_2_Product_norm  uid_td_D15_cUID_2_Product_norm\n",
            "7   uid_td_D1_cUID_1_Product_norm   uid_td_D1_cUID_1_Product_norm\n",
            "8   uid_td_D1_cUID_2_Product_norm   uid_td_D1_cUID_2_Product_norm\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.980482\tvalid_1's auc: 0.922491\n",
            "[400]\ttraining's auc: 0.994725\tvalid_1's auc: 0.933014\n",
            "[600]\ttraining's auc: 0.998444\tvalid_1's auc: 0.93513\n",
            "[800]\ttraining's auc: 0.999475\tvalid_1's auc: 0.935413\n",
            "Early stopping, best iteration is:\n",
            "[803]\ttraining's auc: 0.999484\tvalid_1's auc: 0.93544\n",
            "####################\n",
            "Global AUC 0.935439872184358\n",
            "Week 70 0.9403765678647438 18970\n",
            "Week 71 0.9275626465584624 20726\n",
            "Week 72 0.9305449996107069 20332\n",
            "Week 73 0.9468474405191056 19010\n",
            "Week 74 0.927362236937602 10288\n",
            "####################\n",
            "Features Preformance: 0.935439872184358\n",
            "Diff with previous__: 0.0003100190264497815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "030M0txnN3_k",
        "outputId": "6117b20a-e6f4-4ad9-f9c2-bff68070c362"
      },
      "source": [
        "########################### TransactionAmt clients columns encoding\n",
        "print('TransactionAmt encoding clients columns')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "i_cols = ['TransactionAmt']\n",
        "main_cols = client_cols.copy()\n",
        "\n",
        "for main_col in main_cols:\n",
        "    print(main_col)\n",
        "    for agg_type in ['mean','std']:\n",
        "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
        "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
        "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransactionAmt encoding clients columns\n",
            "uid_td_D1_cUID_1\n",
            "uid_td_D1_cUID_2\n",
            "uid_td_D10_cUID_1\n",
            "uid_td_D10_cUID_2\n",
            "uid_td_D15_cUID_1\n",
            "uid_td_D15_cUID_2\n",
            "card1\n",
            "card2\n",
            "card3\n",
            "card4\n",
            "card5\n",
            "uid1\n",
            "uid2\n",
            "            New columns (including dummy)                           New Features\n",
            "0               card1_TransactionAmt_mean              card1_TransactionAmt_mean\n",
            "1                card1_TransactionAmt_std               card1_TransactionAmt_std\n",
            "2               card2_TransactionAmt_mean              card2_TransactionAmt_mean\n",
            "3                card2_TransactionAmt_std               card2_TransactionAmt_std\n",
            "4               card3_TransactionAmt_mean              card3_TransactionAmt_mean\n",
            "5                card3_TransactionAmt_std               card3_TransactionAmt_std\n",
            "6               card4_TransactionAmt_mean              card4_TransactionAmt_mean\n",
            "7                card4_TransactionAmt_std               card4_TransactionAmt_std\n",
            "8               card5_TransactionAmt_mean              card5_TransactionAmt_mean\n",
            "9                card5_TransactionAmt_std               card5_TransactionAmt_std\n",
            "10               uid1_TransactionAmt_mean               uid1_TransactionAmt_mean\n",
            "11                uid1_TransactionAmt_std                uid1_TransactionAmt_std\n",
            "12               uid2_TransactionAmt_mean               uid2_TransactionAmt_mean\n",
            "13                uid2_TransactionAmt_std                uid2_TransactionAmt_std\n",
            "14  uid_td_D10_cUID_1_TransactionAmt_mean  uid_td_D10_cUID_1_TransactionAmt_mean\n",
            "15   uid_td_D10_cUID_1_TransactionAmt_std   uid_td_D10_cUID_1_TransactionAmt_std\n",
            "16  uid_td_D10_cUID_2_TransactionAmt_mean  uid_td_D10_cUID_2_TransactionAmt_mean\n",
            "17   uid_td_D10_cUID_2_TransactionAmt_std   uid_td_D10_cUID_2_TransactionAmt_std\n",
            "18  uid_td_D15_cUID_1_TransactionAmt_mean  uid_td_D15_cUID_1_TransactionAmt_mean\n",
            "19   uid_td_D15_cUID_1_TransactionAmt_std   uid_td_D15_cUID_1_TransactionAmt_std\n",
            "20  uid_td_D15_cUID_2_TransactionAmt_mean  uid_td_D15_cUID_2_TransactionAmt_mean\n",
            "21   uid_td_D15_cUID_2_TransactionAmt_std   uid_td_D15_cUID_2_TransactionAmt_std\n",
            "22   uid_td_D1_cUID_1_TransactionAmt_mean   uid_td_D1_cUID_1_TransactionAmt_mean\n",
            "23    uid_td_D1_cUID_1_TransactionAmt_std    uid_td_D1_cUID_1_TransactionAmt_std\n",
            "24   uid_td_D1_cUID_2_TransactionAmt_mean   uid_td_D1_cUID_2_TransactionAmt_mean\n",
            "25    uid_td_D1_cUID_2_TransactionAmt_std    uid_td_D1_cUID_2_TransactionAmt_std\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.982033\tvalid_1's auc: 0.923479\n",
            "[400]\ttraining's auc: 0.995691\tvalid_1's auc: 0.931975\n",
            "[600]\ttraining's auc: 0.998951\tvalid_1's auc: 0.934123\n",
            "[800]\ttraining's auc: 0.999691\tvalid_1's auc: 0.934831\n",
            "[1000]\ttraining's auc: 0.999913\tvalid_1's auc: 0.934784\n",
            "Early stopping, best iteration is:\n",
            "[911]\ttraining's auc: 0.999843\tvalid_1's auc: 0.935073\n",
            "####################\n",
            "Global AUC 0.9350726499932759\n",
            "Week 70 0.9386802202542073 18970\n",
            "Week 71 0.9287341946756172 20726\n",
            "Week 72 0.9284983491485859 20332\n",
            "Week 73 0.9470134102379235 19010\n",
            "Week 74 0.9276223169103197 10288\n",
            "####################\n",
            "Features Preformance: 0.9350726499932759\n",
            "Diff with previous__: -0.000367222191082095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZovjtiBRN5lL",
        "outputId": "baf44b90-fbe7-4fde-8ffe-91afde32b4e4"
      },
      "source": [
        "########################### Mark card columns \"outliers\"\n",
        "print('Categorical outliers')\n",
        "## \n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "i_cols = ['TransactionAmt','ProductCD','P_emaildomain','R_emaildomain',]\n",
        "periods = ['DT_M']\n",
        "\n",
        "for period in periods:\n",
        "    for col in i_cols:\n",
        "        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n",
        "        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0).astype(np.int8)\n",
        "\n",
        "        \n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical outliers\n",
            "        New columns (including dummy)                        New Features\n",
            "0   P_emaildomain_catboost_check_DT_M   P_emaildomain_catboost_check_DT_M\n",
            "1       ProductCD_catboost_check_DT_M       ProductCD_catboost_check_DT_M\n",
            "2   R_emaildomain_catboost_check_DT_M   R_emaildomain_catboost_check_DT_M\n",
            "3  TransactionAmt_catboost_check_DT_M  TransactionAmt_catboost_check_DT_M\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.982078\tvalid_1's auc: 0.923681\n",
            "[400]\ttraining's auc: 0.995681\tvalid_1's auc: 0.93326\n",
            "[600]\ttraining's auc: 0.998933\tvalid_1's auc: 0.93497\n",
            "[800]\ttraining's auc: 0.999692\tvalid_1's auc: 0.935572\n",
            "Early stopping, best iteration is:\n",
            "[764]\ttraining's auc: 0.99962\tvalid_1's auc: 0.935635\n",
            "####################\n",
            "Global AUC 0.9356351161172467\n",
            "Week 70 0.9386337795379774 18970\n",
            "Week 71 0.9288584915781568 20726\n",
            "Week 72 0.9300257999972269 20332\n",
            "Week 73 0.9470109589041096 19010\n",
            "Week 74 0.9292052820468619 10288\n",
            "####################\n",
            "Features Preformance: 0.9356351161172467\n",
            "Diff with previous__: 0.0005624661239708173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXSgJ0nhN7Uv",
        "outputId": "8087c250-5c9c-4823-fdd1-ca63ae0a2bec"
      },
      "source": [
        "########################### D Columns Normalize and remove original columns\n",
        "print('D columns transformations')\n",
        "## \n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "# Remove original features\n",
        "# test data will have many unknow values\n",
        "i_cols = ['D'+str(i) for i in range(1,16)]\n",
        "remove_features += i_cols\n",
        "\n",
        "####### Values Normalization\n",
        "i_cols.remove('D1')\n",
        "i_cols.remove('D2')\n",
        "i_cols.remove('D9')\n",
        "periods = ['DT_D']\n",
        "for col in i_cols:\n",
        "    full_df[col] = full_df[col].clip(0)\n",
        "full_df = values_normalization(full_df, periods, i_cols, enc_type='norm')\n",
        "\n",
        "i_cols = ['D1','D2','D9']\n",
        "for col in i_cols:\n",
        "    full_df[col+'_scaled'] = full_df[col]/full_df[col].max()\n",
        "\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D columns transformations\n",
            "   New columns (including dummy)        New Features\n",
            "0             D10_DT_D_std_score  D10_DT_D_std_score\n",
            "1             D11_DT_D_std_score  D11_DT_D_std_score\n",
            "2             D12_DT_D_std_score  D12_DT_D_std_score\n",
            "3             D13_DT_D_std_score  D13_DT_D_std_score\n",
            "4             D14_DT_D_std_score  D14_DT_D_std_score\n",
            "5             D15_DT_D_std_score  D15_DT_D_std_score\n",
            "6                      D1_scaled           D1_scaled\n",
            "7                      D2_scaled           D2_scaled\n",
            "8              D3_DT_D_std_score   D3_DT_D_std_score\n",
            "9              D4_DT_D_std_score   D4_DT_D_std_score\n",
            "10             D5_DT_D_std_score   D5_DT_D_std_score\n",
            "11             D6_DT_D_std_score   D6_DT_D_std_score\n",
            "12             D7_DT_D_std_score   D7_DT_D_std_score\n",
            "13             D8_DT_D_std_score   D8_DT_D_std_score\n",
            "14                     D9_scaled           D9_scaled\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.982209\tvalid_1's auc: 0.923013\n",
            "[400]\ttraining's auc: 0.995907\tvalid_1's auc: 0.931008\n",
            "[600]\ttraining's auc: 0.999086\tvalid_1's auc: 0.933975\n",
            "[800]\ttraining's auc: 0.999767\tvalid_1's auc: 0.934775\n",
            "Early stopping, best iteration is:\n",
            "[882]\ttraining's auc: 0.999863\tvalid_1's auc: 0.934999\n",
            "####################\n",
            "Global AUC 0.9349988449855252\n",
            "Week 70 0.9378465299721095 18970\n",
            "Week 71 0.9265298021364967 20726\n",
            "Week 72 0.9294945127455259 20332\n",
            "Week 73 0.9467113193943764 19010\n",
            "Week 74 0.9308781883843438 10288\n",
            "####################\n",
            "Features Preformance: 0.9349988449855252\n",
            "Diff with previous__: -0.0006362711317214886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAS9ON8NN84C",
        "outputId": "2e4811eb-f3d5-4574-f71e-721f7eed0947"
      },
      "source": [
        "########################### Dist\n",
        "print('Distance normalization')\n",
        "## \n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "i_cols = ['dist1','dist2']\n",
        "main_cols = [\n",
        "             'uid_td_D1_cUID_1',\n",
        "             'card1',\n",
        "            ]\n",
        "\n",
        "\n",
        "for main_col in main_cols:\n",
        "    print(main_col)\n",
        "    for agg_type in ['mean','std']:\n",
        "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
        "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
        "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
        "    \n",
        "    for col in i_cols:\n",
        "        f_std = main_col+'_'+col+'_std'\n",
        "        f_mean = main_col+'_'+col+'_mean'\n",
        "        full_df[main_col+'_'+col+'_norm'] = (full_df[col]-full_df[f_mean])/full_df[f_std]\n",
        "        del full_df[f_mean], full_df[f_std]\n",
        "\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance normalization\n",
            "uid_td_D1_cUID_1\n",
            "card1\n",
            "  New columns (including dummy)                 New Features\n",
            "0              card1_dist1_norm             card1_dist1_norm\n",
            "1              card1_dist2_norm             card1_dist2_norm\n",
            "2   uid_td_D1_cUID_1_dist1_norm  uid_td_D1_cUID_1_dist1_norm\n",
            "3   uid_td_D1_cUID_1_dist2_norm  uid_td_D1_cUID_1_dist2_norm\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.981946\tvalid_1's auc: 0.922745\n",
            "[400]\ttraining's auc: 0.995923\tvalid_1's auc: 0.931731\n",
            "[600]\ttraining's auc: 0.999088\tvalid_1's auc: 0.934337\n",
            "[800]\ttraining's auc: 0.999761\tvalid_1's auc: 0.935391\n",
            "[1000]\ttraining's auc: 0.999941\tvalid_1's auc: 0.93551\n",
            "Early stopping, best iteration is:\n",
            "[1007]\ttraining's auc: 0.999943\tvalid_1's auc: 0.935584\n",
            "####################\n",
            "Global AUC 0.9355834593166266\n",
            "Week 70 0.9385412718738577 18970\n",
            "Week 71 0.9298409888577251 20726\n",
            "Week 72 0.9294413982411064 20332\n",
            "Week 73 0.9472956741167987 19010\n",
            "Week 74 0.9279291163401926 10288\n",
            "####################\n",
            "Features Preformance: 0.9355834593166266\n",
            "Diff with previous__: 0.0005846143311013519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm6DOC01N-nm",
        "outputId": "1b74ed06-e357-4b8f-cb89-9aa718c0f4e7"
      },
      "source": [
        "########################### Count similar transactions per period\n",
        "print('Similar transactions per period')\n",
        "## \n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "periods = ['DT_W','DT_D'] \n",
        "\n",
        "for period in periods:\n",
        "    full_df['TransactionAmt_Product_counts_' + period] =\\\n",
        "        full_df.groupby([period,'ProductCD','TransactionAmt'])['TransactionAmt'].transform('count')\n",
        "    full_df['TransactionAmt_Product_counts_' + period] /= full_df[period+'_total']\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar transactions per period\n",
            "        New columns (including dummy)                        New Features\n",
            "0  TransactionAmt_Product_counts_DT_D  TransactionAmt_Product_counts_DT_D\n",
            "1  TransactionAmt_Product_counts_DT_W  TransactionAmt_Product_counts_DT_W\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.982573\tvalid_1's auc: 0.924481\n",
            "[400]\ttraining's auc: 0.996356\tvalid_1's auc: 0.933441\n",
            "[600]\ttraining's auc: 0.999231\tvalid_1's auc: 0.936153\n",
            "[800]\ttraining's auc: 0.99981\tvalid_1's auc: 0.937201\n",
            "Early stopping, best iteration is:\n",
            "[899]\ttraining's auc: 0.999902\tvalid_1's auc: 0.937398\n",
            "####################\n",
            "Global AUC 0.9373973028683664\n",
            "Week 70 0.9401675379206669 18970\n",
            "Week 71 0.9323021748126351 20726\n",
            "Week 72 0.9325083875766098 20332\n",
            "Week 73 0.9475128334534967 19010\n",
            "Week 74 0.9294423770460063 10288\n",
            "####################\n",
            "Features Preformance: 0.9373973028683664\n",
            "Diff with previous__: 0.0018138435517398577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvMqbpUUOAUA",
        "outputId": "0a7aafb4-dbf4-467f-f200-6bb215cfddd3"
      },
      "source": [
        "########################### Find nunique dates per client\n",
        "print('Nunique dates per client')\n",
        "## \n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "main_cols = {\n",
        "            'uid_td_D1_cUID_1': ['uid_td_D'+str(i) for i in range(2,16) if i!=9] + ['D8','D9'],\n",
        "            }\n",
        "\n",
        "for main_col,i_cols in main_cols.items():\n",
        "    for col in i_cols:\n",
        "        full_df[col+'_catboost_check_'+main_col] = full_df.groupby([main_col])[col].transform('nunique')\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nunique dates per client\n",
            "                 New columns (including dummy)                                New Features\n",
            "0           D8_catboost_check_uid_td_D1_cUID_1          D8_catboost_check_uid_td_D1_cUID_1\n",
            "1           D9_catboost_check_uid_td_D1_cUID_1          D9_catboost_check_uid_td_D1_cUID_1\n",
            "2   uid_td_D10_catboost_check_uid_td_D1_cUID_1  uid_td_D10_catboost_check_uid_td_D1_cUID_1\n",
            "3   uid_td_D11_catboost_check_uid_td_D1_cUID_1  uid_td_D11_catboost_check_uid_td_D1_cUID_1\n",
            "4   uid_td_D12_catboost_check_uid_td_D1_cUID_1  uid_td_D12_catboost_check_uid_td_D1_cUID_1\n",
            "5   uid_td_D13_catboost_check_uid_td_D1_cUID_1  uid_td_D13_catboost_check_uid_td_D1_cUID_1\n",
            "6   uid_td_D14_catboost_check_uid_td_D1_cUID_1  uid_td_D14_catboost_check_uid_td_D1_cUID_1\n",
            "7   uid_td_D15_catboost_check_uid_td_D1_cUID_1  uid_td_D15_catboost_check_uid_td_D1_cUID_1\n",
            "8    uid_td_D2_catboost_check_uid_td_D1_cUID_1   uid_td_D2_catboost_check_uid_td_D1_cUID_1\n",
            "9    uid_td_D3_catboost_check_uid_td_D1_cUID_1   uid_td_D3_catboost_check_uid_td_D1_cUID_1\n",
            "10   uid_td_D4_catboost_check_uid_td_D1_cUID_1   uid_td_D4_catboost_check_uid_td_D1_cUID_1\n",
            "11   uid_td_D5_catboost_check_uid_td_D1_cUID_1   uid_td_D5_catboost_check_uid_td_D1_cUID_1\n",
            "12   uid_td_D6_catboost_check_uid_td_D1_cUID_1   uid_td_D6_catboost_check_uid_td_D1_cUID_1\n",
            "13   uid_td_D7_catboost_check_uid_td_D1_cUID_1   uid_td_D7_catboost_check_uid_td_D1_cUID_1\n",
            "14   uid_td_D8_catboost_check_uid_td_D1_cUID_1   uid_td_D8_catboost_check_uid_td_D1_cUID_1\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.983591\tvalid_1's auc: 0.924083\n",
            "[400]\ttraining's auc: 0.996334\tvalid_1's auc: 0.932582\n",
            "[600]\ttraining's auc: 0.999248\tvalid_1's auc: 0.935071\n",
            "[800]\ttraining's auc: 0.999808\tvalid_1's auc: 0.936124\n",
            "[1000]\ttraining's auc: 0.999955\tvalid_1's auc: 0.936311\n",
            "Early stopping, best iteration is:\n",
            "[900]\ttraining's auc: 0.999905\tvalid_1's auc: 0.936407\n",
            "####################\n",
            "Global AUC 0.9364060793394224\n",
            "Week 70 0.9384730591518299 18970\n",
            "Week 71 0.9318711971431636 20726\n",
            "Week 72 0.9312583124732695 20332\n",
            "Week 73 0.9470833453496756 19010\n",
            "Week 74 0.9287855564424757 10288\n",
            "####################\n",
            "Features Preformance: 0.9364060793394224\n",
            "Diff with previous__: -0.0009912235289439852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CD9UwklOCFN",
        "outputId": "767973e9-7b02-43ba-d60d-c1cb6ca33e62"
      },
      "source": [
        "########################### Email transformation\n",
        "print('Email split')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "p = 'P_emaildomain'\n",
        "r = 'R_emaildomain'\n",
        "\n",
        "full_df['full_email'] = full_df[p].astype(str) +'_'+ full_df[r].astype(str)\n",
        "full_df['email_p_extension'] = full_df[p].apply(lambda x: str(x).split('.')[-1])\n",
        "full_df['email_r_extension'] = full_df[r].apply(lambda x: str(x).split('.')[-1])\n",
        "full_df['email_p_domain'] = full_df[p].apply(lambda x: str(x).split('.')[0])\n",
        "full_df['email_r_domain'] = full_df[r].apply(lambda x: str(x).split('.')[0])\n",
        "\n",
        "i_cols = ['P_emaildomain','R_emaildomain',\n",
        "          'full_email',\n",
        "          'email_p_extension','email_r_extension',\n",
        "          'email_p_domain','email_r_domain']\n",
        "\n",
        "full_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email split\n",
            "  New columns (including dummy)       New Features\n",
            "0                email_p_domain     email_p_domain\n",
            "1             email_p_extension  email_p_extension\n",
            "2                email_r_domain     email_r_domain\n",
            "3             email_r_extension  email_r_extension\n",
            "4                    full_email         full_email\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.982567\tvalid_1's auc: 0.924148\n",
            "[400]\ttraining's auc: 0.996307\tvalid_1's auc: 0.933213\n",
            "[600]\ttraining's auc: 0.99922\tvalid_1's auc: 0.935671\n",
            "[800]\ttraining's auc: 0.999795\tvalid_1's auc: 0.936018\n",
            "[1000]\ttraining's auc: 0.999949\tvalid_1's auc: 0.936216\n",
            "Early stopping, best iteration is:\n",
            "[1012]\ttraining's auc: 0.999954\tvalid_1's auc: 0.936295\n",
            "####################\n",
            "Global AUC 0.9362955394479312\n",
            "Week 70 0.9381353594567502 18970\n",
            "Week 71 0.9312575290817968 20726\n",
            "Week 72 0.9302946432920185 20332\n",
            "Week 73 0.9482524152847873 19010\n",
            "Week 74 0.928360834104704 10288\n",
            "####################\n",
            "Features Preformance: 0.9362955394479312\n",
            "Diff with previous__: -0.00011053989149123478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzTwXktlODra",
        "outputId": "a35782c4-f356-4001-d270-4ce11b8dc0b2"
      },
      "source": [
        "########################### Device info and identity\n",
        "print('Identity sets')\n",
        "saved_state = list(full_df)\n",
        "####\n",
        "\n",
        "########################### Device info\n",
        "identity_df['DeviceInfo'] = identity_df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
        "identity_df['DeviceInfo_device'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "identity_df['DeviceInfo_version'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
        "    \n",
        "########################### Device info 2\n",
        "identity_df['id_30'] = identity_df['id_30'].fillna('unknown_device').str.lower()\n",
        "identity_df['id_30_device'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "identity_df['id_30_version'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
        "    \n",
        "########################### Browser\n",
        "identity_df['id_31'] = identity_df['id_31'].fillna('unknown_device').str.lower()\n",
        "identity_df['id_31_device'] = identity_df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "    \n",
        "########################### Merge Identity columns\n",
        "temp_df = full_df[['TransactionID']]\n",
        "temp_df = temp_df.merge(identity_df, on=['TransactionID'], how='left')\n",
        "del temp_df['TransactionID']\n",
        "full_df = pd.concat([full_df,temp_df], axis=1)\n",
        "  \n",
        "i_cols = [\n",
        "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
        "          'id_30','id_30_device','id_30_version',\n",
        "          'id_31','id_31_device',\n",
        "          'id_33','DeviceType'\n",
        "         ]\n",
        "\n",
        "####### Global Self frequency encoding\n",
        "full_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n",
        "\n",
        "####\n",
        "if MAKE_TESTS:\n",
        "    print(get_new_columns(saved_state))\n",
        "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
        "####"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identity sets\n",
            "   New columns (including dummy)        New Features\n",
            "0                     DeviceInfo          DeviceInfo\n",
            "1              DeviceInfo_device   DeviceInfo_device\n",
            "2             DeviceInfo_version  DeviceInfo_version\n",
            "3                     DeviceType          DeviceType\n",
            "4                          id_01               id_01\n",
            "5                          id_02               id_02\n",
            "6                          id_03               id_03\n",
            "7                          id_04               id_04\n",
            "8                          id_05               id_05\n",
            "9                          id_06               id_06\n",
            "10                         id_07               id_07\n",
            "11                         id_08               id_08\n",
            "12                         id_09               id_09\n",
            "13                         id_10               id_10\n",
            "14                         id_11               id_11\n",
            "15                         id_12               id_12\n",
            "16                         id_13               id_13\n",
            "17                         id_14               id_14\n",
            "18                         id_15               id_15\n",
            "19                         id_16               id_16\n",
            "20                         id_17               id_17\n",
            "21                         id_18               id_18\n",
            "22                         id_19               id_19\n",
            "23                         id_20               id_20\n",
            "24                         id_21               id_21\n",
            "25                         id_22               id_22\n",
            "26                         id_23               id_23\n",
            "27                         id_24               id_24\n",
            "28                         id_25               id_25\n",
            "29                         id_26               id_26\n",
            "30                         id_27               id_27\n",
            "31                         id_28               id_28\n",
            "32                         id_29               id_29\n",
            "33                         id_30               id_30\n",
            "34                  id_30_device        id_30_device\n",
            "35                 id_30_version       id_30_version\n",
            "36                         id_31               id_31\n",
            "37                  id_31_device        id_31_device\n",
            "38                         id_32               id_32\n",
            "39                         id_33               id_33\n",
            "40                       id_33_0             id_33_0\n",
            "41                       id_33_1             id_33_1\n",
            "42                         id_34               id_34\n",
            "43                         id_35               id_35\n",
            "44                         id_36               id_36\n",
            "45                         id_37               id_37\n",
            "46                         id_38               id_38\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttraining's auc: 0.983346\tvalid_1's auc: 0.925521\n",
            "[400]\ttraining's auc: 0.996683\tvalid_1's auc: 0.933884\n",
            "[600]\ttraining's auc: 0.999402\tvalid_1's auc: 0.936642\n",
            "[800]\ttraining's auc: 0.999875\tvalid_1's auc: 0.937366\n",
            "[1000]\ttraining's auc: 0.999977\tvalid_1's auc: 0.937692\n",
            "Early stopping, best iteration is:\n",
            "[1087]\ttraining's auc: 0.99999\tvalid_1's auc: 0.938006\n",
            "####################\n",
            "Global AUC 0.9380056708349994\n",
            "Week 70 0.9397613451773045 18970\n",
            "Week 71 0.9350726470182537 20726\n",
            "Week 72 0.9322773714790755 20332\n",
            "Week 73 0.9479147801009372 19010\n",
            "Week 74 0.9302503488344496 10288\n",
            "####################\n",
            "Features Preformance: 0.9380056708349994\n",
            "Diff with previous__: 0.0017101313870682144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "h4vcsc3uOFRM",
        "outputId": "b2127654-0621-473e-b7cc-7afbebcbcb72"
      },
      "source": [
        "########################### Export\n",
        "full_df.to_pickle('baseline_full_df.pkl')\n",
        "\n",
        "remove_features_df = pd.DataFrame(remove_features, columns=['features_to_remove'])\n",
        "remove_features_df.to_pickle('baseline_remove_features.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6bcbf7f60b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m########################### Export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfull_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline_full_df.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mremove_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features_to_remove'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mremove_features_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline_remove_features.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_df' is not defined"
          ]
        }
      ]
    }
  ]
}